<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://numisveinsson.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://numisveinsson.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-09T13:51:33+00:00</updated><id>https://numisveinsson.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Making a Vascular Model Repository — Design Decisions, Metadata, and Workflows</title><link href="https://numisveinsson.com/blog/2025/vmr/" rel="alternate" type="text/html" title="Making a Vascular Model Repository — Design Decisions, Metadata, and Workflows"/><published>2025-10-19T00:00:00+00:00</published><updated>2025-10-19T00:00:00+00:00</updated><id>https://numisveinsson.com/blog/2025/vmr</id><content type="html" xml:base="https://numisveinsson.com/blog/2025/vmr/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In many of my recent projects on <strong>patient-specific vascular modeling</strong> — from segmentation to simulation — I’ve run into the same problem over and over: managing the data.<br/> Not just storing it, but doing so in a way that’s traceable, reproducible, and easy for others (and algorithms) to use.</p> <p>That challenge became the motivation for creating a <strong>vascular model repository</strong> <a href="https://vascularmodel.com">(see VMR here)</a>— a structured system for organizing, versioning, and sharing vascular data with clear metadata and consistent workflows. What follows is an overview of the design decisions and practical trade-offs involved in building it.</p> <hr/> <h2 id="traceability-and-persistent-identifiers">Traceability and Persistent Identifiers</h2> <p>One of the most important principles is <strong>traceability</strong>.<br/> Once data enters the repository, its full history should stay visible — including updates or replacements over time.</p> <p>Each dataset gets a <strong>persistent identifier (DOI)</strong>, ensuring that anyone citing or using it can always trace back to the exact version referenced.<br/> When updates are made, the old versions remain archived and linked through a clear version history.<br/> That version lineage is what gives the data real scientific credibility.</p> <hr/> <h2 id="naming-structure-and-standardization">Naming Structure and Standardization</h2> <p>Naming conventions sound trivial until you try to merge data from multiple sources — hospitals, labs, and research groups, each with their own systems.</p> <p>The repository uses a <strong>standardized naming pattern</strong> to keep things consistent:<br/> <code class="language-plaintext highlighter-rouge">{StudyName}{PatientID}{Region}{Modality}{Version}</code><br/> Example: <code class="language-plaintext highlighter-rouge">CCTA001_LAD_CT_v2</code></p> <p>But older datasets don’t always fit. For those, we wrote <strong>mapping scripts</strong> that automatically translate legacy names into the new format.<br/> It’s a constant balance — enforcing standards without breaking compatibility with existing archives.</p> <hr/> <h2 id="hosting-and-backup">Hosting and Backup</h2> <p>Reliability starts with where the data lives.<br/> The repository runs on a <strong>dedicated research server</strong> with <strong>backups</strong>, so no single point of failure can wipe anything out.</p> <p>We should also experiment with <strong>cloud-based object storage</strong> (S3-compatible) for long-term archiving, which would make scaling and collaboration much easier while keeping version control intact.</p> <hr/> <h2 id="workflow-for-adding-new-data">Workflow for Adding New Data</h2> <p>Adding data should be simple, not chaotic.<br/> The submission process follows three main steps:</p> <ol> <li><strong>Upload</strong> — contributors upload their model files (segmentations, meshes, or simulation results).</li> <li><strong>Metadata Form</strong> — they fill in key details: study name, region, modality, preprocessing steps, and license.</li> <li><strong>Validation Check</strong> — automated scripts confirm that the files and metadata follow the required format and naming rules.</li> </ol> <p>This keeps the workflow lightweight, but structured enough to guarantee completeness and consistency.</p> <hr/> <h2 id="quality-control">Quality Control</h2> <p>Every dataset should go through <strong>quality control (QC)</strong> before it’s officially added.<br/> That includes:</p> <ul> <li>Checking geometry for intersections or topological errors</li> <li>Visual review of segmentations</li> <li>Verifying metadata consistency</li> <li>Optional short simulation runs to test validity</li> </ul> <p>Contributors should also be able to run the same QC scripts locally before submitting, which helps maintain a uniform quality standard across contributions.</p> <hr/> <h2 id="automated-and-ai-based-validation">Automated and AI-Based Validation</h2> <p>Manual review doesn’t scale forever.<br/> That’s where <strong>AI-assisted validation</strong> comes in — lightweight automated checks that flag potential issues, such as:</p> <ul> <li>Incomplete or disconnected vessel trees</li> <li>Unusual geometries</li> <li>Low-confidence segmentation areas</li> </ul> <p>These don’t replace human review but act as an early filter, keeping the process fast while maintaining trust in the data.</p> <hr/> <h2 id="data-structure-and-ml-compatibility">Data Structure and ML Compatibility</h2> <p>A big design goal was making data not just organized, but <strong>usable</strong>.<br/> Consistent folder hierarchies, standardized formats (<code class="language-plaintext highlighter-rouge">.nii.gz</code>, <code class="language-plaintext highlighter-rouge">.vtp</code>, <code class="language-plaintext highlighter-rouge">.stl</code>), and structured JSON metadata mean that ML or coding workflows can plug in directly.</p> <p>With this structure, training pipelines can automatically parse metadata, load the correct models, and run experiments — a major step toward reproducible research at scale.</p> <hr/> <h2 id="looking-ahead">Looking Ahead</h2> <p>This repository isn’t just a storage project; it’s an attempt to make vascular data <strong>reliable, transparent, and usable</strong>.<br/> Having structured, traceable datasets doesn’t just help with reproducibility — it builds trust and speeds up collaboration across groups.</p> <p>Next steps include:</p> <ul> <li>Finalizing the metadata schema and submission interface</li> <li>Releasing the first curated models with DOIs</li> </ul> <hr/>]]></content><author><name>Numi Sveinsson Cepero</name></author><summary type="html"><![CDATA[Building a reproducible, standardized vascular model repository with traceability, quality control, and support for automated workflows.]]></summary></entry><entry><title type="html">SeqSeg, Now a Pip Installable Package</title><link href="https://numisveinsson.com/blog/2025/seqseg_pip/" rel="alternate" type="text/html" title="SeqSeg, Now a Pip Installable Package"/><published>2025-05-01T00:00:00+00:00</published><updated>2025-05-01T00:00:00+00:00</updated><id>https://numisveinsson.com/blog/2025/seqseg_pip</id><content type="html" xml:base="https://numisveinsson.com/blog/2025/seqseg_pip/"><![CDATA[<h2 id="1-pip-installation">1. Pip Installation</h2> <p><a href="https://pypi.org/project/seqseg/">SeqSeg</a> is now available as a pip installable package! This makes it easier to install and use in your projects. To install SeqSeg, simply run the following command in your terminal:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>seqseg
</code></pre></div></div> <p>This will install the latest version of SeqSeg along with all its dependencies.</p> <h2 id="2-tutorial">2. Tutorial</h2> <p>To get started with SeqSeg, you can follow the tutorial provided in the <a href="https://github.com/numisveinsson/SeqSeg/blob/main/seqseg/tutorial/tutorial.md">SeqSeg GitHub repository</a>. The tutorial has data and code examples to help you understand how to use SeqSeg for medical image segmentation tasks. The data is the same as <a href="https://simvascular.github.io/">SimVascular</a>. Main steps in the tutorial include:</p> <ol> <li><strong>Seed Point Definition</strong>: Define seed points in the images where you want to start the segmentation.</li> <li><strong>Weight Downloading</strong>: Download the pre-trained weights for the model.</li> <li><strong>Segmentation</strong>: Run the segmentation algorithm on your images using the defined seed points and downloaded weights.</li> <li><strong>Visualization</strong>: Visualize the segmentation results to assess the performance of the model.</li> </ol>]]></content><author><name>Numi Sveinsson Cepero</name></author><category term="data-science"/><summary type="html"><![CDATA[A tutorial on setting up SeqSeg for medical image segmentation]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://numisveinsson.com/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://numisveinsson.com/blog/2025/plotly</id><content type="html" xml:base="https://numisveinsson.com/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">SeqSeg, From New Dataset to Training to Inference</title><link href="https://numisveinsson.com/blog/2025/seqseg_setup/" rel="alternate" type="text/html" title="SeqSeg, From New Dataset to Training to Inference"/><published>2025-01-15T00:00:00+00:00</published><updated>2025-01-15T00:00:00+00:00</updated><id>https://numisveinsson.com/blog/2025/seqseg_setup</id><content type="html" xml:base="https://numisveinsson.com/blog/2025/seqseg_setup/"><![CDATA[<h2 id="1-data-preparation-and-structure">1. Data Preparation and Structure</h2> <p>The first step in using <a href="https://github.com/numisveinsson/SeqSeg/"><code class="language-plaintext highlighter-rouge">SeqSeg</code></a> is to preprocess your data.</p> <p>We require the following data:</p> <ul> <li>A directory containing the images named <code class="language-plaintext highlighter-rouge">images</code></li> <li>A directory containing the masks named <code class="language-plaintext highlighter-rouge">truths</code></li> <li>A directory containing the centerlines named <code class="language-plaintext highlighter-rouge">centerlines</code> <ul> <li>as <code class="language-plaintext highlighter-rouge">.vtp</code> files</li> </ul> </li> </ul> <p>A few things to note:</p> <ul> <li>The images and masks and centerlines should have the same name</li> <li>If you need to extract centerlines from masks, you can use the <code class="language-plaintext highlighter-rouge">SeqSeg/centerlines.py</code> script, or use VMTK or other tools <ul> <li>Centerlines must contain radius information in the <code class="language-plaintext highlighter-rouge">.vtp</code> file</li> </ul> </li> <li>Make sure that the masks, images and centerlines align correctly, for example open and view together in an image viewer e.g. <code class="language-plaintext highlighter-rouge">Paraview</code></li> <li>Make sure the images contain origin, spacing and direction information in the metadata <ul> <li>This is important for correct alignment before centerline extraction</li> </ul> </li> </ul> <h2 id="2-data-preprocessing-for-training">2. Data Preprocessing (for Training)</h2> <p>The next step is to preprocess the data for training. SeqSeg requires a model trained on local patches, so we need to extract patches from the images and masks based on centerlines.</p> <p>The repository for this is <a href="https://github.com/numisveinsson/BloodVesselML3D"><code class="language-plaintext highlighter-rouge">BloodVesselML3D</code></a> and the script is <code class="language-plaintext highlighter-rouge">gather_sampling_data_parallel.py</code>. This requires the following arguments:</p> <ul> <li><code class="language-plaintext highlighter-rouge">config</code> - the configuration file for the dataset, which you must change to match your data</li> </ul> <p>Next, we must change the naming structure to match nnU-Net. This is done with the <code class="language-plaintext highlighter-rouge">dataset_dirs/create_nnunet.py</code> script. The new data can be output anywhere, but we recommend directly into the nnU-Net Raw directory.</p> <h2 id="3-training">3. Training</h2> <p>The next step is to train the model. This is done with the specific nnU-Net commands, which are detailed in the <a href="https://github.com/MIC-DKFZ/nnUNet"><code class="language-plaintext highlighter-rouge">nnU-Net</code></a> repository.</p> <p>This requires two commands (see <a href="https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/how_to_use_nnunet.md">documentation</a> for more details):</p> <ul> <li><code class="language-plaintext highlighter-rouge">prepocessing</code> command</li> <li><code class="language-plaintext highlighter-rouge">train</code> command</li> </ul> <p>You can train:</p> <ul> <li>2D models</li> <li>3D low resolution models</li> <li>3D full resolution models</li> </ul> <h2 id="4-inference">4. Inference</h2> <p>The final step is to run <a href="https://github.com/numisveinsson/SeqSeg/"><code class="language-plaintext highlighter-rouge">SeqSeg</code></a> inference on new data. This is done with the <code class="language-plaintext highlighter-rouge">SeqSeg/seqseg.py</code> script. You need direct access to the directory containing the images and seed points, and another containing the trained model weights.</p>]]></content><author><name>Numi Sveinsson Cepero</name></author><category term="data-science"/><summary type="html"><![CDATA[A tutorial on setting up SeqSeg for medical image segmentation]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://numisveinsson.com/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://numisveinsson.com/blog/2024/photo-gallery</id><content type="html" xml:base="https://numisveinsson.com/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">ParaView Tutorial</title><link href="https://numisveinsson.com/blog/2024/paraview/" rel="alternate" type="text/html" title="ParaView Tutorial"/><published>2024-09-24T00:00:00+00:00</published><updated>2024-09-24T00:00:00+00:00</updated><id>https://numisveinsson.com/blog/2024/paraview</id><content type="html" xml:base="https://numisveinsson.com/blog/2024/paraview/"><![CDATA[<h3 id="data-visualization">Data Visualization</h3> <p>Data visualization is an essential tool for scientists and engineers to explore and communicate complex data. Visualization allows us to gain insights into our data, identify patterns, and communicate our findings effectively. ParaView is a powerful scientific visualization tool that enables users to visualize and analyze large datasets in 2D and 3D. It supports a wide range of data formats and provides a variety of visualization and analysis tools. Examples of data that can be visualized in ParaView include computational fluid dynamics simulations, geospatial data, medical imaging data, and more.</p> <p>In this tutorial, we’ll cover the basics of using ParaView for scientific visualization. We’ll explore how to load data, visualize data in 2D and 3D, apply filters for data analysis, and export visualizations for publication.</p> <h3 id="what-is-paraview">What is ParaView?</h3> <p>ParaView is an open-source, multi-platform data analysis and visualization application. It is designed to handle large datasets and provides a range of visualization and analysis tools for scientific and engineering applications. ParaView supports a variety of data formats, including structured and unstructured grids, point clouds, and images.</p> <div class="l-body"> <p><img src="/assets/img/paraview.png" alt="ParaView Logo" width="700"/></p> </div> <h3 id="getting-started-with-paraview">Getting Started with ParaView</h3> <p>To get started with ParaView, you can download the latest version of the software from the <a href="https://www.paraview.org/download/">ParaView website</a>. ParaView is available for Windows, macOS, and Linux operating systems. Once you have downloaded and installed ParaView, you can launch the application and start exploring its features.</p> <h3 id="loading-data">Loading Data</h3> <p>ParaView supports a wide range of data formats, including VTK, XML, CSV, and more. To load data into ParaView, follow these steps:</p> <ol> <li>Launch ParaView.</li> <li>Click on the “Open” button in the toolbar.</li> <li>Browse to the location of your data file and select it.</li> <li>Click “OK” to load the data.</li> </ol> <h3 id="visualizing-data">Visualizing Data</h3> <p>Once you have loaded data into ParaView, you can visualize it in 2D and 3D. ParaView provides a variety of visualization options, including contour plots, volume rendering, and streamlines. To visualize data in ParaView, follow these steps:</p> <ol> <li>Select the data you want to visualize in the Pipeline Browser.</li> <li>Click on the “Apply” button in the toolbar to apply the default visualization.</li> <li>Use the Display panel to customize the visualization, such as changing the color map, opacity, and lighting.</li> </ol> <h3 id="filters-and-data-analysis">Filters and Data Analysis</h3> <p>ParaView provides a range of filters for data analysis and manipulation. Filters allow you to extract information from your data, apply transformations, and perform calculations.</p> <p>To apply a filter in ParaView, follow these steps:</p> <ol> <li>Select the data you want to filter in the Pipeline Browser.</li> <li>Click on the “Filters” menu and select the desired filter.</li> <li>Configure the filter parameters in the Properties panel.</li> <li>Click “Apply” to apply the filter to the data.</li> </ol> <h3 id="favorite-filters">Favorite Filters</h3> <p>Some of my favorite filters in ParaView include:</p> <ul> <li><strong>Contour</strong>: Useful for visualizing isocontours of scalar data.</li> <li><strong>Clip</strong>: Great for cutting away parts of the dataset to reveal internal structures.</li> <li><strong>Slice</strong>: Useful for extracting 2D slices from 3D datasets.</li> <li><strong>Threshold</strong>: Handy for filtering data based on scalar values.</li> <li><strong>Stream Tracer</strong>: Useful for visualizing flow fields and streamlines.</li> <li><strong>Glyph</strong>: Great for visualizing vector fields with glyphs.</li> <li><strong>Warp by Scalar</strong>: Useful for deforming the dataset based on scalar values. This is great for visualizing deformation fields.</li> </ul> <h3 id="visualizing-3d-medical-image-data">Visualizing 3D Medical Image Data</h3> <p>ParaView is a powerful tool for visualizing 3D medical image data, such as MRI and CT scans. It provides a range of tools for volume rendering, segmentation, and surface extraction. To visualize 3D medical image data in ParaView, follow these steps:</p> <ol> <li>Load the medical image data into ParaView.</li> <li>Apply a volume rendering filter to visualize the data in 3D.</li> <li>Choose isosurface as blend mode and adjust the opacity and color map.</li> <li>Use the segmentation tools to extract regions of interest.</li> <li>Apply surface extraction filters to create 3D surface meshes.</li> </ol> <h3 id="saving-and-exporting-data">Saving and Exporting Data</h3> <p>Once you have created a visualization in ParaView, you can save it in a variety of formats for publication and sharing. ParaView supports a range of file formats, including images, videos, and 3D models.</p> <p>To save a visualization in ParaView, follow these steps:</p> <ol> <li>Click on the “File” menu and select “Save Screenshot” to save an image of the visualization.</li> <li>Click on the “File” menu and select “Save Animation” to save a video of the visualization.</li> <li>Click on the “File” menu and select “Export Scene” to save the visualization as a ParaView state file.</li> </ol> <h3 id="conclusion">Conclusion</h3> <p>ParaView is a powerful scientific visualization tool that enables users to explore and analyze complex datasets in 2D and 3D. In this tutorial, we covered the basics of using ParaView for data visualization, including loading data, visualizing data, applying filters, and exporting visualizations. ParaView is a versatile tool that is widely used in scientific research, engineering, and data analysis. I hope this tutorial has provided you with a solid foundation for using ParaView in your own work. Happy visualizing!</p>]]></content><author><name>Numi Sveinsson Cepero</name></author><category term="data-science"/><summary type="html"><![CDATA[A beginner-friendly tutorial on using ParaView for scientific visualization]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://numisveinsson.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://numisveinsson.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://numisveinsson.com/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://numisveinsson.com/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://numisveinsson.com/blog/2024/tabs</id><content type="html" xml:base="https://numisveinsson.com/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="3f2d76b7-1020-4563-810e-34abb11ccbc0" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="3f2d76b7-1020-4563-810e-34abb11ccbc0" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="ec93a594-4ae5-466e-b9b4-540422c9793d" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="ec93a594-4ae5-466e-b9b4-540422c9793d" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="f9fdf728-e62b-45a6-9632-b4afa09cfc47" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="f9fdf728-e62b-45a6-9632-b4afa09cfc47" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://numisveinsson.com/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://numisveinsson.com/blog/2024/typograms</id><content type="html" xml:base="https://numisveinsson.com/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://numisveinsson.com/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://numisveinsson.com/blog/2024/post-citation</id><content type="html" xml:base="https://numisveinsson.com/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry></feed>